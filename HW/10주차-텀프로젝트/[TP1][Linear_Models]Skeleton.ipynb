{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGreHU47ZqyC"
      },
      "source": [
        "## 1. 선형 회귀(Linear Regression)\n",
        "\n",
        "* 본 텀프로젝트의 목적은 **경사 하강법(Gradient Descent)를 이용하여 선형 회귀 문제를 해결**하는 것\n",
        "\n",
        "* 사이킷런 [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)을 참조하면 선형 회귀 모델에 대한 설명을 아래 사진과 같이 확인 가능\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1KboYt4oBL1sZk3glRgQQPh9YjIBgLOso\" width=\"900\">\n",
        "</center>\n",
        "\n",
        "\n",
        "* 즉, 선형 회귀는 학습 데이터셋 $\\{( x_i,y_i)|1\\le i \\le n\\}$ (단, $ x_i=(x_{i1},\\cdots,x_{ik})^{\\rm T}$)에 대한 학습모델을 다음과 같이 각 특성값에 대해 선형이 되도록 가정하는 판별모델\n",
        "\n",
        "$$\\hat y = \\beta_0 +\\beta_1  x_1+\\cdots +\\beta_k  x_k$$\n",
        "\n",
        " * 편향(또는 절편) $\\beta_0$와 가중치 $\\beta_i\\, (1\\le i\\le k)$를 합쳐서 모델 파라미터라고 정의\n",
        "\n",
        " * $k$차원 특성벡터 $ x=(x_1,\\cdots,x_k)^{\\rm T}$를 $ x=(1,x_1,\\cdots,x_k)^{\\rm T}$로 쓰고(즉, $x_0=1$), $\\beta = (\\beta_0,\\beta_1,\\cdots,\\beta_k)^{\\rm T}$로 나타내면 위 모델은 $\\hat y = h_{\\beta}(x)=\\beta^{\\rm T} x$로 간단히 표기 가능\n",
        "\n",
        " * 모든 훈련샘플의 특성 벡터를 $n\\times (k+1)$행렬 $ X=(x_{ij})$로 나타낼 때, \n",
        "$$\\hat {y} = (\\hat y_1,\\cdots,\\hat y_n)^{\\rm T} = X \\beta$$\n",
        "\n",
        " * $k=1$일 때를 **단순선형회귀 모델**, $k>1$일 때를 **다중선형회귀 모델**이라 정의\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA29RorfH48A"
      },
      "outputs": [],
      "source": [
        "# 선형 회귀 문제를 풀기 위해\n",
        "# 랜덤하게 데이터 샘플 생성 \n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = 2 * np.random.rand(100,1)\n",
        "y = 4 + 3 * X + np.random.randn(100,1)\n",
        "\n",
        "plt.plot(X,y, 'b.')\n",
        "plt.xlabel(\"$x_1$\", fontsize=10)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=10)\n",
        "plt.axis([0, 2, 0, 15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY5UXqZYpnJI"
      },
      "outputs": [],
      "source": [
        "# 각 특성벡터의 첫 번째 좌표에 bias에 대응되는 1을 추가하여 Xb로 수정 \n",
        "# 여러 가지 경사 하강법을 직접 구현해 볼 때 사용\n",
        "\n",
        "Xb = np.column_stack((np.ones((100,1)), X)) # bias에 대응 할 수 있도록 1을 추가\n",
        "\n",
        "X_new = np.array([[0],[2]]) # 위에서 만든 데이터가 아닌 새로운 데이터 샘플 x_new를 정의하여 회귀 계수를 구할 때 마다 테스트에 사용\n",
        "Xb_new = np.column_stack((np.ones((2,1)), X_new)) # bias에 대응 할 수 있도록 1을 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfZqxKNDX6iK"
      },
      "outputs": [],
      "source": [
        "# sklearn의 LinearRegression을 활용하여 회귀 계수 구해보기\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "beta = np.array([lin_reg.intercept_, lin_reg.coef_[0]],dtype=float) # 회귀 계수 받아오기\n",
        "\n",
        "y_predict = lin_reg.predict(X_new) # X_new에 대해서 y값을 선형 회귀 모델을 이용하여 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t36341pYppDx"
      },
      "outputs": [],
      "source": [
        "# 모델의 예측을 그래프에 나타내기 \n",
        "plt.plot(X, y, 'b.')\n",
        "plt.plot(X_new, y_predict, 'r-')\n",
        "plt.xlabel(\"$x_1$\", fontsize=10)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=10)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.title(\"Linear regression with Scikit Learn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U00KpNaIaEqS"
      },
      "source": [
        "## 1-1. 회귀 계수 결정법 (Direct Solution)\n",
        "\n",
        "\n",
        "* 선형회귀에서 주로 사용하는 Mean Squared Error(MSE) 손실 함수를 사용하면 아래로 볼록한(Convex) 형태의 손실함수를 가짐\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\\\\n",
        "L(\\beta) & = \\left\\|\\hat{y}-y \\right\\|_{2}^{2}\\\\\n",
        "& = \\left\\|\\beta X-y \\right\\|_{2}^{2}\\\\\n",
        "& = (\\beta X-y)^{\\rm T}(\\beta X-y)\\\\\n",
        "& = \\beta^{\\rm T}X^{\\rm T}X\\beta-\\beta^{\\rm T}X^{\\rm T}y-y^{\\rm T}X\\beta+y^{\\rm T}y \\\\\n",
        "\\end{aligned}$$\n",
        "\n",
        "\n",
        "* 아래로 볼록한(Convex) 형태의 손실함수의 최소값은 미분이 0이 되는 지점($\\nabla_{\\beta} L(\\beta)=0$)을 통해 명시적인 해를 구할 수 있음\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\\\\n",
        "\\nabla_{\\beta} L(\\beta)& =X^{\\rm T}X\\beta+X^{\\rm T}X\\beta-2X^{\\rm T}y=0\\\\\n",
        "& \\Rightarrow  2X^{\\rm T}X\\beta=2X^{\\rm T}y\\\\\n",
        "& \\Rightarrow  X^{\\rm T}X\\beta=X^{\\rm T}y\\\\\n",
        "\\end{aligned}$$\n",
        "\n",
        "\n",
        "\n",
        "$$\\therefore \\beta=(X^{\\rm T}X)^{-1}X^{\\rm T}y$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UlHd3dNQ954"
      },
      "outputs": [],
      "source": [
        "####### Empty Module.1 #######\n",
        "# Direct Solution을 통해서 최적의 Beta를 찾아보세요\n",
        "beta_direct =                                        # 위의 수식을 참고하여 최적의 beta를 찾아보세요\n",
        "y_predict_direct =                                   # 찾은 최적의 beta를 가지고 X_new에 대한 y값을 예측 해보세요\n",
        "##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3p92WmM5mXG"
      },
      "outputs": [],
      "source": [
        "# 모델의 예측을 그래프에 나타내기 \n",
        "plt.plot(X, y, 'b.')\n",
        "plt.plot(X_new, y_predict_direct, 'r-')\n",
        "plt.xlabel(\"$x_1$\", fontsize=10)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=10)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.title(\"Linear regression with Direct Search(Least Square)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usdrF3_xpkER"
      },
      "source": [
        "## 1-2. 회귀 계수 결정법 (Numerical Search)\n",
        "\n",
        "* 경사하강법(gradient descent)와 같은 반복적인 방식으로 선형회귀 계수를 구할 수 있음\n",
        "\n",
        "* 경사(gradient)란? **\"임의의 지점에서 함수의 최대 증가 방향\"**을 의미\n",
        "$$ \\text{경사(gradient)} ⇒ \\nabla_{\\beta}L(\\beta)=[\\frac{\\partial L(\\beta)}{\\partial \\beta_{0}},\\cdots ,\\frac{\\partial L(\\beta)}{\\partial \\beta_{k}}]\\in R^{k+1}$$\n",
        "\n",
        "\n",
        "* 결국 경사하강법은 최대 증가의 반대 방향인 최대 감소의 방향으로 회귀 계수를 업데이트 하겠다는 것\n",
        "$$ \\text{경사하강법(gradient descent)} ⇒ \\beta^{(t+1)}=\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta)$$\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1PulS6p9zKdUsV9g0KPPP68ThA39gJirG\" width=\"480\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV7sSWzcqAio"
      },
      "source": [
        "#### 배치 경사 하강법(Batch Gradient Descent) : 구현 \n",
        "\n",
        "* 파라미터를 업데이트 할 때마다 모든 학습 데이터를 사용하여 cost function의 gradient를 계산\n",
        "\n",
        "* Vanilla Gradient Descent라 불림\n",
        "\n",
        "* 모든 학습 데이터를 사용하기 때문에 gradient의 방향성은 정확하지만 연산이 오래걸려 학습 효율이 좋지 못함\n",
        "\n",
        "$$\\nabla_{\\beta} L(\\beta) = \\dfrac 2 n  X^{\\rm T}( X\\beta- y) \\; : \\; \\text{gradient}$$\n",
        "\n",
        "$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V7Ec6Trp9bQ"
      },
      "outputs": [],
      "source": [
        "beta_bgd_path = []\n",
        "eta = 0.1 # 학습률 \n",
        "n_epochs = 1000 # epoch 수 \n",
        "n = 100 # 샘플수 \n",
        "\n",
        "beta_bgd = np.random.randn(2,1)  # 무작위로 beta 초깃값 설정 \n",
        "\n",
        "####### Empty Module.2 #######\n",
        "# Batch Gradient Descent를 통해서 최적의 Beta를 찾아보세요\n",
        "for iteration in range(n_epochs):\n",
        "    gradients =                                               # 배치 경사 하강법에 해당하는 gradient를 계산하세요 \n",
        "    beta_bgd =                                                # update rule에 따라서 beta_bgd를 update 하세요\n",
        "    beta_bgd_path.append(beta_bgd)\n",
        "\n",
        "y_predict_bgd =                                               # 얻은 회귀 계수를 가지고 X_new에 대해서 y값을 예측하세요\n",
        "##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyIWGDuG5rsx"
      },
      "outputs": [],
      "source": [
        "# 모델의 예측을 그래프에 나타내기 \n",
        "plt.plot(X, y, 'b.')\n",
        "plt.plot(X_new, y_predict_bgd, 'r-')\n",
        "plt.xlabel(\"$x_1$\", fontsize=10)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=10)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.title(f\"Linear regression with BGD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-pcHhsIqNeB"
      },
      "source": [
        "#### 확률적 경사 하강법(Stochastic Gradient Descent) : 구현 \n",
        "\n",
        "* 파라미터를 업데이트 할 때, 무작위로 샘플링된 학습 데이터를 하나씩만 이용하여 cost function의 gradient를 계산\n",
        "\n",
        "* 모델을 자주 업데이트 하며, 성능 개선 정도를 빠르게 확인 가능\n",
        "\n",
        "* Local minima에 빠질 가능성을 줄일 수 있음\n",
        "\n",
        "$$\\nabla_{\\beta} L(\\beta) = 2 x_{i}(x_{i}^{\\rm T}\\beta- y) \\; : \\; \\text{gradient}$$\n",
        "\n",
        "$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h679VfimqK5P"
      },
      "outputs": [],
      "source": [
        "beta_sgd_path = []\n",
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # 학습 스케줄 하이퍼파라미터 \n",
        "n = 100         # 샘플 수 \n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0/(t+t1)\n",
        "\n",
        "beta_sgd = np.random.randn(2,1)  # beta 무작위 초기화 \n",
        "\n",
        "####### Empty Module.3 #########\n",
        "# Stochastic Gradient Descent를 통해서 최적의 Beta를 찾아보세요\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(n):\n",
        "        eta = learning_schedule(epoch * n + i)\n",
        "\n",
        "        random_idx =                                      # 0 ~ n-1 까지 랜덤하게 인덱스 선택\n",
        "        tx =                                              # random_idx를 활용해 샘플 하나 선택\n",
        "        ty =                                              # random_idx를 활용해 샘플 하나 선택\n",
        "        gradients =                                       # 확률적 경사 하강법에 해당하는 gradient를 계산하세요 \n",
        "        beta_sgd =                                        # update rule에 따라서 beta_sgd를 update 하세요\n",
        "\n",
        "        beta_sgd_path.append(beta_sgd)\n",
        "\n",
        "y_predict_sgd =                                           # 얻은 회귀 계수를 가지고 X_new에 대해서 y값을 예측하세요\n",
        "##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzywjap25v-N"
      },
      "outputs": [],
      "source": [
        "# 모델의 예측을 그래프에 나타내기 \n",
        "plt.plot(X, y, 'b.')\n",
        "plt.plot(X_new, y_predict_sgd, 'r-')\n",
        "plt.xlabel(\"$x_1$\", fontsize=10)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=10)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.title(f\"Linear regression with SGD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXZmc6MgvgCY"
      },
      "source": [
        "#### 미니 배치 경사 하강법(Mini Batch Gradient Descent) : 구현 \n",
        "\n",
        "* 파라미터를 업데이트 할 때마다 일정량의 일부 데이터를 무작위로 뽑아 cost function의 gradient를 계산\n",
        "\n",
        "* Batch Gradient Descent와 Stochastic Gradient Descent 개념의 혼합\n",
        "\n",
        "* SGD의 노이즈를 줄이면서, GD의 전체 배치보다 효율적\n",
        "\n",
        "$$\\nabla_{\\beta}L(\\beta)=\\frac{2}{m}\\sum_{i=1}^{m} x_{i}(x_{i}^{\\rm T}\\beta-y) \\; : \\; \\text{gradient}$$\n",
        "\n",
        "$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmoK2KeuqVfE"
      },
      "outputs": [],
      "source": [
        "beta_mgd_path = []\n",
        "\n",
        "n_epochs = 50\n",
        "minibatch_size = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "beta_mgd = np.random.randn(2,1)  # 랜덤 초기화\n",
        "\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "\n",
        "####### Empty Module.4 #########\n",
        "# Mini Batch Gradient Descent를 통해서 최적의 Beta를 찾아보세요.\n",
        "for epoch in range(n_epochs):\n",
        "    shuffled_indices = np.random.permutation(n)\n",
        "    Xb_shuffled = Xb[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, n, minibatch_size):\n",
        "        t += 1\n",
        "        eta = learning_schedule(t)\n",
        "\n",
        "        tx =                                                               # minibatch_size를 활용해 batch 단위로 샘플링\n",
        "        ty =                                                               # minibatch_size를 활용해 batch 단위로 샘플링\n",
        "        gradients =                                                        # 확률적 경사 하강법에 해당하는 gradient를 계산하세요 \n",
        "        beta_mgd =                                                         # update rule에 따라서 beta_mgd를 update 하세요\n",
        "\n",
        "        beta_mgd_path.append(beta_mgd)\n",
        "\n",
        "y_predict_mgd =                                                             # 얻은 회귀 계수를 가지고 X_new에 대해서 y값을 예측하세요\n",
        "##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbSfX2M551vG"
      },
      "outputs": [],
      "source": [
        "# 모델의 예측을 그래프에 나타내기 \n",
        "plt.plot(X, y, 'b.')\n",
        "plt.plot(X_new, y_predict_mgd, 'r-')\n",
        "plt.xlabel(\"$x_1$\", fontsize=10)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=10)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.title(f\"Linear regression with MGD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcMc-bqVqXxq"
      },
      "source": [
        "#### 파라미터 공간에 표시된 경사 하강법의 경로 비교 \n",
        "\n",
        "* 앞에서 계산해둔 `beta_bgd_path`, `beta_sgd_path`, `beta_mgd_path`에 각각 순차적으로 저장된 $(\\beta_0,\\beta_1)$값을 이용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06lCzkfpqZFm"
      },
      "outputs": [],
      "source": [
        "beta_bgd_path = np.array(beta_bgd_path)\n",
        "beta_sgd_path = np.array(beta_sgd_path)\n",
        "beta_mgd_path = np.array(beta_mgd_path)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(beta_sgd_path[:, 0], beta_sgd_path[:, 1], \"r-x\", linewidth=1.5, label=\"Stochastic\")\n",
        "plt.plot(beta_mgd_path[:, 0], beta_mgd_path[:, 1], \"g-+\", linewidth=1, label=\"Mini-batch\")\n",
        "plt.plot(beta_bgd_path[:, 0], beta_bgd_path[:, 1], \"b-o\", linewidth=1.5, label=\"Batch\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.xlabel(r\"$\\beta_0$\", fontsize=10)\n",
        "plt.ylabel(r\"$\\beta_1$   \", fontsize=10, rotation=0)\n",
        "plt.axis([2.5, 4.25, 2.3, 3.9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFGXcLkFxOZB"
      },
      "outputs": [],
      "source": [
        "print(\"{:36s} : {}, {}\".format(\"사이킷런 회귀 계수\",beta[0],beta[1]))\n",
        "print(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Direct Search)\",beta_direct[0],beta_direct[1]))\n",
        "print(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Batch Descent)\",beta_bgd[0],beta_bgd[1]))\n",
        "print(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Stochastic Descent)\",beta_sgd[0],beta_sgd[1]))\n",
        "print(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Mini-Batch Descent)\",beta_mgd[0],beta_mgd[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtwRoIwqINmA"
      },
      "source": [
        "\n",
        "## 2. 선형 분류(Linear Classification)\n",
        "\n",
        "* 본 텀프로젝트의 목적은 **경사 하강법(Gradient Descent)를 이용하여 선형 분류 문제를 해결**하는 것\n",
        "\n",
        "* 사이킷런 [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)을 참조하면 선형 분류 모델에 대한 설명을 아래 사진과 같이 확인 가능\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1hfO_iU1tDg2pyFiyw2h5AeLi3Ro-p4-X\" width=\"900\">\n",
        "</center>\n",
        "\n",
        "* 레이블이 $1$, $0$인 두 개의 클래스에 대한 분류문제에서 샘플이 특정 클래스에 속할 확률을 추정하는 지도학습의 한 가지 (Binary case)\n",
        "\n",
        "\n",
        "\n",
        "* 선형회귀 모델과 같이 입력 특성의 가중치의 합(편향 포함) ${\\beta}^{\\rm T} x = \\beta_0+\\beta_1 x_1+\\cdots +\\beta_nx_n$을 계산한 다음 시그모이드 함수(sigmoid) $\\sigma(t)=\\dfrac{1}{1+\\exp(-t)}$를 취한 값 $\\sigma({\\beta}^{\\rm T} x)$를 ${\\rm P}(Y=1|X=x)$에 대한 추정값 $\\hat p(x)$로 추정하는 모델.   \n",
        "\n",
        "\n",
        "* 즉, 모델 파라미터 ${\\beta}=(\\beta_0,\\cdots,\\beta_n)^{\\rm T}$에 대한 로지스틱 회귀 모델을 $h_{{\\beta}}$라 할 때\n",
        "$$ $$\n",
        "$\\quad \\quad \n",
        "\\hat p(x) = h_{{\\beta}}(x)= \\sigma({\\beta}^{\\rm T}x)\n",
        "=\\dfrac{1}{1+\\exp(-{\\beta}^{\\rm T}x)}=\\dfrac{1}{1+\\exp\\bigl(-(\\beta_0+\\beta_1x_1+\\cdots+\\beta_n x_n)\\bigr)} \n",
        "$  <span style=\"color:blue\">$\\cdots\\cdots$ (1)</span>\n",
        "\n",
        "\n",
        "* <span style=\"color:blue\"> 로지스틱 회귀 모델을 통한 레이블의 예측 :</span>    \n",
        "\n",
        "* 샘플 $x$가 양성 클래스(y=1)에 속할 확률 $\\hat p(x)=h_{{\\beta}}(x)$를 추정한 후 다음과 같이 예측 $\\hat y$를 구함 \n",
        "$$ $$\n",
        "$$\n",
        "\\hat y = \\begin{cases} 0 & \\text{ if }\\hat p(x)<0.5\\\\ 1 &\\text{ if }\\hat p(x)\\ge0.5\n",
        "\\end{cases}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InBXpUVBcmVm"
      },
      "outputs": [],
      "source": [
        "# 시그모이드 함수 시각화\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "xlist = np.linspace(-10,10,1000)\n",
        "ylist = sigmoid(xlist)\n",
        "\n",
        "plt.plot(xlist,ylist, label=r\"$\\sigma (t)=\\dfrac{1}{1+\\exp(-t)}$\")\n",
        "\n",
        "plt.plot([-10,10],[0.5,0.5], 'r--', linewidth=0.6)\n",
        "plt.plot([-10,10],[1,1], 'r--', linewidth=0.6)\n",
        "plt.plot([-10,10],[0,0], 'r--', linewidth=0.6)\n",
        "plt.plot([0,0],[0,1],'r--', linewidth=0.6)\n",
        "\n",
        "plt.legend(loc=\"upper left\", fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22utuT0dzGlx"
      },
      "source": [
        "**참고) 시그모이드 함수 미분** \n",
        "> $$\n",
        "\\begin{aligned}\n",
        "&\\\\\n",
        "\\sigma (x)& =\\frac{1}{1+e^{-x}} \\\\\n",
        "\\sigma' (x)& =\\frac{0\\cdot \\left( 1+e^{-x}\\right)  -1\\cdot (e^{-x}\\times -1)}{(1+e^{-x})^{2}}\\\\\n",
        "& =  \\frac{e^{-x}}{(1+e^{-x})^{2}} =\\frac{1-1+e^{-x}}{(1+e^{-x})^{2}} =\\frac{1+e^{-x}}{\\left( 1+e^{-x}\\right)^{2}  } -\\frac{1}{\\left( 1+e^{-x}\\right)^{2}  }\\\\\n",
        "& =  \\frac{1}{1+e^{-x}} \\left( 1-\\frac{1}{1+e^{-x}} \\right)  =\\sigma (x)\\left( 1-\\sigma (x)\\right)\\\\\n",
        "\\end{aligned}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZwugMMmeB0t"
      },
      "source": [
        "## 2-1. 로지스틱 회귀모델의 손실 함수 \n",
        "\n",
        "* 훈련 데이터셋 $\\{(x_i,y_i)|1\\le i \\le n\\}$이 주어질 때, 다음과 같이 정의되는 로지스틱 회귀의 비용함수 \n",
        "$$ $$\n",
        "$L(\\beta) = -\\dfrac 1 n \\sum_{i=1}^n \\left(y_i \\ln p_i + (1-y_i)\\ln (1-p_i)\\right)\\quad $ (단, $p_i= \\sigma(\\beta^{\\rm T}x_i$) <span style=\"color:blue\">\n",
        "$$ $$\n",
        "가 최소가 되는 모델 파라미터 $\\beta$를 구하는 것   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWTRUWTyuseH"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "# 이진 분류를 위해 iris 데이터에서 두가지 클래스만을 사용\n",
        "\n",
        "sepal_len = iris['data'][:100,0]\n",
        "sepal_wid = iris['data'][:100,1]\n",
        "labels = iris['target'][:100]\n",
        "\n",
        "sepal_len -= np.mean(sepal_len)\n",
        "sepal_wid -= np.mean(sepal_wid)\n",
        "\n",
        "plt.scatter(sepal_len, \n",
        "            sepal_wid,\n",
        "            c=labels,\n",
        "            cmap=plt.cm.Paired)\n",
        "plt.xlabel(\"sepal length\")\n",
        "plt.ylabel(\"sepal width\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLbK47FexEg-"
      },
      "outputs": [],
      "source": [
        "# X, y 설정\n",
        "X = np.stack([sepal_len, sepal_wid], axis=1)\n",
        "y = labels\n",
        "y = np.expand_dims(y,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cubdSYOwba_"
      },
      "outputs": [],
      "source": [
        "def plot_sep(w1, w2, title, color='green'):\n",
        "\n",
        "    plt.scatter(sepal_len, \n",
        "                sepal_wid,\n",
        "                c=labels,\n",
        "                cmap=plt.cm.Paired)\n",
        "    plt.title(title)\n",
        "    plt.ylim([-1.5,1.5])\n",
        "    plt.xlim([-1.5,2])\n",
        "    plt.xlabel(\"sepal length\")\n",
        "    plt.ylabel(\"sepal width\")\n",
        "    if w2 != 0:\n",
        "        m = -w1/w2\n",
        "        t = 1 if w2 > 0 else -1\n",
        "        plt.plot(\n",
        "            [-1.5,2.0], \n",
        "            [-1.5*m, 2.0*m], \n",
        "            '-y', \n",
        "            color=color)\n",
        "        plt.fill_between(\n",
        "            [-1.5, 2.0],\n",
        "            [m*-1.5, m*2.0],\n",
        "            [t*1.5, t*1.5],\n",
        "            alpha=0.2,\n",
        "            color=color)\n",
        "    if w2 == 0: # decision boundary is vertical\n",
        "        t = 1 if w1 > 0 else -1\n",
        "        plt.plot([0, 0],\n",
        "                 [-1.5, 2.0],\n",
        "                 '-y',\n",
        "                color=color)\n",
        "        plt.fill_between(\n",
        "            [0, 2.0*t],\n",
        "            [-1.5, -2.0],\n",
        "            [1.5, 2],\n",
        "            alpha=0.2,\n",
        "            color=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwBZvbewwicM"
      },
      "outputs": [],
      "source": [
        "# sklearn의 LogisticRegression 활용하여 회귀 계수 구해보기\n",
        "import sklearn.linear_model\n",
        "\n",
        "model = sklearn.linear_model.LogisticRegression(fit_intercept=False,random_state=42)\n",
        "model.fit(X,y)\n",
        "plot_sep(model.coef_[0][0], model.coef_[0][1], \"Linear Classification with Logistic Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd_VHihY78ts"
      },
      "source": [
        "## 2-2. 회귀 계수 결정법 (Numerical Search)\n",
        "\n",
        "\n",
        "* 비용함수 $L(\\beta)$는 $\\beta$에 대해 아래로 볼록한(Convex) 함수이므로 최솟값이 존재함을 보장할 수 있지만, Direct Search 처럼 해를 구하는 공식은 없음  \n",
        "\n",
        "* 경사하강법 또는 다른 최적화 알고리즘(BFGS, Newton ...)을 이용하여 해의 근삿값을 구함 (`LogisticRegression`의 [solver](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)참고)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* 배치 경사하강법을 적용할 때 비용함수에 대한 그래디언트 벡터 $\\nabla_{\\beta}L(\\beta)$ : 각 $i$ ($1\\le i\\le n)$에 대해 $\\nabla_{\\beta}L(\\beta)$의 $j$번째 성분\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\\\\n",
        "\\dfrac{\\partial L(\\beta)}{\\partial \\beta_j} & = -\\frac{1}{n} \\sum^{n}_{i=1} [y_{i}\\cdot \\left( \\frac{1}{\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  } \\right)  \\cdot \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\left( 1-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\right)  \\cdot x^{(n)}_{i}]-[\\left( 1-y_{i}\\right)  \\cdot \\left( \\frac{1}{1-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  } \\right)  \\cdot \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\cdot \\left( 1-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\right)  \\cdot x^{(n)}_{i}]\\\\\n",
        "& = -\\frac{1}{n} \\sum^{n}_{i=1} [y_{i}\\cdot \\left( 1-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\right)  \\cdot x^{(n)}_{i}]+[\\left( 1-y_{i}\\right)  \\cdot \\left( -1\\right)  \\cdot \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\cdot x^{(n)}_{i}] \\\\\n",
        "& = -\\frac{1}{n} \\sum^{n}_{i=1} \\left[ y_{i}-y_{i}\\cdot \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  -\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  +y_{i}\\cdot \\sigma \\left( \\beta^{T} x_{i}\\right)  \\right]  \\cdot x^{(n)}_{i} \\\\\n",
        "& = -\\frac{1}{n} \\sum^{n}_{i=1} \\left[ y_{i}-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\right]  \\cdot x^{(n)}_{i} \\\\\n",
        "& = \\frac{1}{n} \\sum^{n}_{i=1} \\left[ \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  -y_{i}\\right]  \\cdot x^{(n)}_{i} \\\\\n",
        "\\end{aligned}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtqYQKw1fVo"
      },
      "source": [
        "#### 배치 경사 하강법(Batch Gradient Descent) : 구현 \n",
        "\n",
        "* 파라미터를 업데이트 할 때마다 모든 학습 데이터를 사용하여 cost function의 gradient를 계산\n",
        "\n",
        "* Vanilla Gradient Descent라 불림\n",
        "\n",
        "* 모든 학습 데이터를 사용하기 때문에 gradient의 방향성은 정확하지만 연산이 오래걸려 학습 효율이 좋지 못함\n",
        "\n",
        "$$\\nabla_{\\beta} L(\\beta) = \\dfrac 1 n  X^{\\rm T}( \\sigma (X\\beta)- y) \\; : \\; \\text{gradient}$$\n",
        "\n",
        "$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiqqBVzfxolQ"
      },
      "outputs": [],
      "source": [
        "beta_bgd_path = list()\n",
        "eta = 0.1 # 학습률 \n",
        "n_epochs = 500 # epoch 수 \n",
        "n = 100 # 샘플수 \n",
        "\n",
        "np.random.seed(42)\n",
        "beta_bgd = np.random.randn(2,1) \n",
        "\n",
        "####### Empty Module.5 #########\n",
        "# Batch Gradient Descent를 통해서 최적의 Beta를 찾아보세요\n",
        "for iteration in range(n_epochs):\n",
        "    gradients =                                             # 배치 경사 하강법에 해당하는 gradient를 계산하세요 \n",
        "    beta_bgd =                                              # update rule에 따라서 beta_bgd를 update 하세요\n",
        "    beta_bgd_path.append(beta_bgd)         \n",
        "###############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKQBnoPf7Gub"
      },
      "outputs": [],
      "source": [
        "# 결정 경계 시각화\n",
        "plot_sep(beta_bgd[0][0], beta_bgd[1][0], \"Linear Classification with BGD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i64JBT5o1i8F"
      },
      "source": [
        "#### 확률적 경사 하강법(Stochastic Gradient Descent) : 구현 \n",
        "\n",
        "* 파라미터를 업데이트 할 때, 무작위로 샘플링된 학습 데이터를 하나씩만 이용하여 cost function의 gradient를 계산\n",
        "\n",
        "* 모델을 자주 업데이트 하며, 성능 개선 정도를 빠르게 확인 가능\n",
        "\n",
        "* Local minima에 빠질 가능성을 줄일 수 있음\n",
        "\n",
        "$$\\nabla_{\\beta} L(\\beta) = x(\\sigma(x^{\\rm T}\\beta)- y) \\; : \\; \\text{gradient}$$\n",
        "\n",
        "$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myP7pdRu22N9"
      },
      "outputs": [],
      "source": [
        "beta_sgd_path = []\n",
        "n_epochs = 500\n",
        "t0, t1 = 5, 10  # 학습 스케줄 하이퍼파라미터 \n",
        "n = 100         # 샘플 수 \n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0/(t+t1)\n",
        "    \n",
        "np.random.seed(42)\n",
        "beta_sgd = np.random.randn(2,1)  # beta 무작위 초기화 \n",
        "\n",
        "####### Empty Module.6 #########\n",
        "# Stochastic Gradient Descent를 통해서 최적의 Beta를 찾아보세요\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(n):\n",
        "        eta = learning_schedule(epoch * n + i)\n",
        "\n",
        "        random_idx =                                                 # 0 ~ n-1 까지 랜덤하게 인덱스 선택\n",
        "        tx =                                                         # random_idx를 활용해 샘플 하나 선택\n",
        "        ty =                                                         # random_idx를 활용해 샘플 하나 선택\n",
        "        gradients =                                                  # 확률적 경사 하강법에 해당하는 gradient를 계산하세요 \n",
        "        beta_sgd =                                                   # update rule에 따라서 beta_mgd를 update 하세요\n",
        "\n",
        "        beta_sgd_path.append(beta_sgd)\n",
        "##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKDS2WXm7LyC"
      },
      "outputs": [],
      "source": [
        "# 결정 경계 시각화\n",
        "plot_sep(beta_sgd[0][0], beta_sgd[1][0], \"Linear Classification with SGD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk9h0ueu1meF"
      },
      "source": [
        "#### 미니 배치 경사 하강법(Mini Batch Gradient Descent) : 구현 \n",
        "\n",
        "* 파라미터를 업데이트 할 때마다 일정량의 일부 데이터를 무작위로 뽑아 cost function의 gradient를 계산\n",
        "\n",
        "* Batch Gradient Descent와 Stochastic Gradient Descent 개념의 혼합\n",
        "\n",
        "* SGD의 노이즈를 줄이면서, GD의 전체 배치보다 효율적\n",
        "\n",
        "$$\\nabla_{\\beta}L(\\beta)=\\frac{1}{m}\\sum_{i=1}^{m} x_{i}(\\sigma (x_{i}^{\\rm T}\\beta)-y) \\; : \\; \\text{gradient}$$\n",
        "\n",
        "$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIY-X8xD4AHu"
      },
      "outputs": [],
      "source": [
        "beta_mgd_path = []\n",
        "\n",
        "n_epochs = 50\n",
        "minibatch_size = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "beta_mgd = np.random.randn(2,1)  # 랜덤 초기화\n",
        "\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "\n",
        "####### Empty Module.8#########\n",
        "for epoch in range(n_epochs):\n",
        "    shuffled_indices = np.random.permutation(n)\n",
        "    X_shuffled = X[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, n, minibatch_size):\n",
        "        t += 1\n",
        "        eta = learning_schedule(t)\n",
        "\n",
        "        tx =                                  # minibatch_size를 활용해 batch 단위로 샘플링\n",
        "        ty =                                  # minibatch_size를 활용해 batch 단위로 샘플링\n",
        "        gradients =                           # 미니 배치 경사 하강법에 해당하는 gradient를 계산하세요 \n",
        "        beta_mgd =                            # update rule에 따라서 beta_mgd를 update 하세요\n",
        "        \n",
        "        beta_mgd_path.append(beta_mgd)\n",
        "############################### "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgy56KAW7PoV"
      },
      "outputs": [],
      "source": [
        "# 결정 경계 시각화\n",
        "plot_sep(beta_mgd[0][0], beta_mgd[1][0], \"Linear Classification with MGD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aXdKZwv4eBY"
      },
      "source": [
        "#### 파라미터 공간에 표시된 경사 하강법의 경로 비교 \n",
        "\n",
        "* 앞에서 계산해둔 `beta_bgd_path`, `beta_sgd_path`, `beta_mgd_path`에 각각 순차적으로 저장된 $(\\beta_0,\\beta_1)$값을 이용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PZjrxUL4daJ"
      },
      "outputs": [],
      "source": [
        "beta_bgd_path = np.array(beta_bgd_path)\n",
        "beta_sgd_path = np.array(beta_sgd_path)\n",
        "beta_mgd_path = np.array(beta_mgd_path)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(beta_sgd_path[:, 0], beta_sgd_path[:, 1], \"r-x\", linewidth=0.5, label=\"Stochastic\")\n",
        "plt.plot(beta_mgd_path[:, 0], beta_mgd_path[:, 1], \"g-+\", linewidth=1, label=\"Mini-batch\")\n",
        "plt.plot(beta_bgd_path[:, 0], beta_bgd_path[:, 1], \"b-o\", linewidth=1., label=\"Batch\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.xlabel(r\"$\\beta_0$\", fontsize=10)\n",
        "plt.ylabel(r\"$\\beta_1$   \", fontsize=10, rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WODt4oKpxBX-"
      },
      "outputs": [],
      "source": [
        "print(\"{:36s} : {}, {}\".format(\"사이킷런 회귀 계수\",model.coef_[0][0], model.coef_[0][1]))\n",
        "print(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Batch Descent)\",beta_bgd[0][0], beta_bgd[1][0]))\n",
        "print(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Stochastic Descent)\",beta_sgd[0][0], beta_sgd[1][0]))\n",
        "print(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Mini-Batch Descent)\",beta_mgd[0][0], beta_mgd[1][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB_a8W-9qmNl"
      },
      "source": [
        "# 보고서\n",
        "\n",
        "1. 사이킷런에 구현되어 있는 [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)은 이번 텀프로젝트에서 구현한 경사 하강법(Gradient Descent) 기반의 방식과 어떤 차이가 있는지 자유롭게 서술하세요. [1점]\n",
        "\n",
        "    * 회귀 계수를 추정하는 과정에서의 다른 부분을 위주로 서술하시면 됩니다.\n",
        "\n",
        "2. 사이킷런에 구현되어 있는 [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)은 이번 텀프로젝트에서 구현한 경사 하강법(Gradient Descent) 기반의 방식과 어떤 차이가 있는지 자유롭게 서술하세요. [1점]\n",
        "\n",
        "    * 회귀 계수를 추정하는 과정에서의 다른 부분을 위주로 서술하시면 됩니다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
