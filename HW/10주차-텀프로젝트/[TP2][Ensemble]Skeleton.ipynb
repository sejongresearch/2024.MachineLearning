{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 1. 결정 트리(Decision Tree)\n","\n","* 본 텀프로젝트의 목적은 **회귀 결정 트리**를 구현하는 것\n","\n","* 사이킷런 [tree](https://scikit-learn.org/stable/modules/tree.html)을 참조하면 결정 나무 모델에 대한 설명을 아래 사진과 같이 확인 가능\n","\n","<center>\n","<img src=\"https://drive.google.com/uc?id=17gQ6QQyMrrt2pNIRhy8kCLdQuICOxGrP\" width=\"900\">\n","</center>\n","\n","* 결정 트리(Decision Tree)는 스무고개 게임과 유사하여 룰 기반의 프로그램에 적용되는 `if`, `else`를 자동으로 찾아내(분할 규칙) 예측을 위한 알고리즘\n","\n","* 결국 결정트리를 생성하는 것은 주어진 특성공간을 분할 규칙에 따라 분할하는 것과 같음\n","\n","* 학습 데이터 $D=\\{(x_i,y_i)|1\\le i \\le m\\}$의 특성벡터 $x_i\\, (1\\le i \\le m)$를 포함하는 특성공간  $\\mathcal X$를 어떤 <span style=\"color:blue\"> 분할 규칙(splitting rule)</span>에 따라 겹치지 않는 작은 영역 $\\mathcal R_i$로 나눔\n","$$\\mathcal X = \\mathcal R_1 \\cup \\mathcal R_2 \\cup \\cdots \\cup \\mathcal R_N$$\n","\n","\n","* 회귀문제인지 분류문제인지에 따라, 임의의 샘플벡터 $x$에 대해 다음과 같이 예측 \n","> * 회귀의 경우: 샘플 $x$가 속하는 작은 영역 $\\mathcal R_i$에 대해, **이 영역에 속하는 훈련샘플 $x_j$의 $y_j$값의 평균**으로 예측 \n","$$\\hat y = \\dfrac 1 {r_i} \\sum_{x_j \\in \\mathcal R_i}y_j, \\quad (r_i=\\bigl|\\{(x_j,y_j)\\in D|x_j\\in \\mathcal R_i\\}\\bigr|)$$\n","$$ $$\n","> * 분류의 경우: 샘플 $x$가 속하는 작은 영역 $\\mathcal R_i$에 대해, $\\mathcal R_i$에 속하는 훈련샘플에 대한 레이블 중 가장 많이 나타나는 레이블  \n","$$ $$\n"],"metadata":{"id":"5ZE7jS7QPYLN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkAoZtOvPRUK"},"outputs":[],"source":["from sklearn.datasets import make_friedman1\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","\n","import numpy as np "]},{"cell_type":"markdown","source":["#### 1-1) 트리구조를 구현하기 위해 트리노드 클래스를 구현  \n","\n","* 클래스의 속성와 메소드 \n","> * 결정트리에서의 각 노드는 특성공간 $\\mathcal X$의 부분집합에 대응하므로 이 부분집합에 속하는 훈련 데이터셋 `self.X`와 레이블 `self.y`가 필요  \n","> * 결정트리에서 노드가 위치하는 깊이: `self.depth`  \n","> * 노드에서 분할규칙 $s_{j}^{x_{ij}}$를 결정하는 속성 `self.j`와 기준값 $x_{ij}$를 나타내는 `self.xi`  \n","> * 자식노드: `self.left`, `self.right`  \n","> * 노드에 대응되는 예측함수: `self.predictor`\n","> * 노드에 대응하는 집합 $\\mathcal A$에서의 예측함수 $g^{\\mathcal A}$를 이용하여 $\\mathcal A$에 속하는 훈련샘플에 대한 **SSE(Sum of Squares for Error)**를 계산하는 함수 `CalculateLoss`  \n"],"metadata":{"id":"_J57ISvEPc_H"}},{"cell_type":"code","source":["class TNode:\n","    def __init__(self,depth, X, y):\n","\n","        self.depth = depth                      # 트리 max depth\n","        self.X = X                              # train_X (Feature)\n","        self.y = y                              # train_y (Label)\n","        self.xi = None                          # 분할 인덱스\n","        self.left = None                        # 왼쪽 자식 노드\n","        self.right = None                       # 오른쪽 자식 노드\n","        self.predictor = None                   # 예측 함수\n","\n","    def CalculateLoss(self):\n","        if len(self.y)==0:\n","            return 0\n","        else:\n","            ####### Empty Module.1 #######\n","            return                              # SSE Loss를 직접 구현\n","            ##############################"],"metadata":{"id":"h5yRnFJ-Peu0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1-2. 결정트리(Decision Tree) 생성 함수 \n","\n","* 주어진 노드에 대응되는 영역 $\\mathcal A$를 분할하여 훈련 데이터셋에 대한 손실값이 최소가 되는 분할규칙 $s_j^{x_{ij}}$를 찾아서 `j`,`xi`를 반환해주는 함수 `CalculateOptimalSplit`를 구현 \n","\n","\n","* `CalculateOptimalSplit`함수를 이용하여 구한 분할규칙에 따라 $\\mathcal A$의 분할 $\\mathcal A_T$, $\\mathcal A_F$에 대응되는 노드를 생성하기 위해, 현재 노드에 대응되는 영역 $\\mathcal A$에 속하는 훈련 데이터셋 `X`,`y`를 영역 $\\mathcal A_T$에 속하는 훈련 데이터셋 `Xt`,`yt`와 영역 $\\mathcal A_F$에 속하는 훈련 데이터셋 `Xf`,`yf`로 나누어주는 함수 `DataSplit`를  구현 "],"metadata":{"id":"rjfmpYSWPraS"}},{"cell_type":"code","source":["def DataSplit(X, y, j, xi):\n","    ####### Empty Module.2 #######\n","    ids =                           # xi보다 작거나 같은 X들의 인덱스 추출\n","    Xt =                            # ids가 True인 X들만 샘플링\n","    Xf =                            # ids가 False인 X들만 샘플링\n","    yt =                            # ids가 True인 y들만 샘플링\n","    yf =                            # ids가 False인 y들만 샘플링\n","    ##############################\n","    return Xt, yt, Xf, yf\n","\n","def CalculateOptimalSplit(node):\n","    X = node.X\n","    y = node.y\n","    best_feature = 0\n","    bext_xi = X[0, best_feature]\n","    best_split_val = node.CalculateLoss()\n","    \n","    m,n = X.shape\n","    \n","    for j in range(0,n):\n","        for i in range(0,m):\n","            xi = X[i,j]\n","            Xt, yt, Xf, yf = DataSplit(X,y,j,xi)\n","            tmpt = TNode(0, Xt, yt)\n","            tmpf = TNode(0, Xf, yf)\n","            loss_t = tmpt.CalculateLoss()\n","            loss_f = tmpf.CalculateLoss()\n","            curr_val = loss_t + loss_f\n","            ####### Empty Module.3 #######\n","            if (curr_val < best_split_val): \n","                best_split_val =             # loss 업데이트\n","                best_feature =               # best_feature 업데이트\n","                best_xi =                    # best_xi 업데이트\n","            ##############################\n","\n","    return best_feature, best_xi"],"metadata":{"id":"lanTeimhPwMd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1-3. 결정트리(Decision Tree) 생성 함수 \n","\n","* 현재 노드의 자식노드를 설정하고, `Construct_Subtree`를 재귀적으로 사용함으로써 트리구조를 구현 \n","> * 미리 설정된 `depth`에 도달하거나, 더 이상 분할할 수 없을 때(즉, `self.X` 또는 `self.y`의 개수가 $1$) 리프노드에 대한 예측함수를 설정."],"metadata":{"id":"BVzSDE_WPiCq"}},{"cell_type":"code","source":["def Construct_Subtree(node, max_depth):\n","    if (node.depth == max_depth or len(node.y) == 1): # node의 깊이가 max_depth에 도달했거나 리프 노드일 때\n","        ####### Empty Module.4 #######\n","        node.predictor =                   # node 내부에 있는 y값들의 평균을 활용하여 예측 수행\n","        ##############################\n","    else:\n","        j, xi = CalculateOptimalSplit(node)                \n","        node.j = j\n","        node.xi = xi\n","        Xt, yt, Xf, yf = DataSplit(node.X, node.y, j, xi)  \n","\n","        if (len(yt)>0): \n","            ####### Empty Module.5 #######\n","            node.left =                          # TNode를 활용하여 새로운 왼쪽 자식 노드 구축\n","            Construct_Subtree(,)                 # Construct_Subtree를 활용하여 왼쪽 자식 노드에 대한 Subtree 구축\n","            ##############################\n","        if (len(yf)>0): \n","            ####### Empty Module.6 #######\n","            node.right =                         # TNode를 활용하여 새로운 오른쪽 자식 노드 구축\n","            Construct_Subtree(,)                 # Construct_Subtree를 활용하여 오른쪽 자식 노드에 대한 Subtree 구축\n","            ##############################\n","\n","    return node"],"metadata":{"id":"U3AWn_lpPgZh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1-4.결정트리(Decision Tree) 예측기 구현  \n","\n","* 학습된 결정트리를 이용하여 예측기 `Predict`를 재귀적으로 구현 "],"metadata":{"id":"3sI_e9CHP3qK"}},{"cell_type":"code","source":["def Predict(X, node):\n","    ####### Empty Module.7 #######\n","    if (node.right == None and node.left != None):\n","        return                                         # 재귀적으로 왼쪽 node에 대해서 Predict 함수 호출\n","    \n","    if (node.right != None and node.left == None):\n","        return                                         # 재귀적으로 오른쪽 node에 대해서 Predict 함수 호출\n","    \n","    if (node.right == None and node.left == None):\n","        return node.predictor \n","    else:\n","        if (X[node.j] <= node.xi):\n","            return                                     # 재귀적으로 왼쪽 node에 대해서 Predict 함수 호출\n","        else:\n","            return                                     # 재귀적으로 오른쪽 node에 대해서 Predict 함수 호출\n","    ##############################"],"metadata":{"id":"17kPNf_jP5cK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1-5. 실습 데이터셋 생성 및 결정트리 실습 \n","\n","* `sklearn.datasets.make_friedman1` 함수를 이용하여 특성 개수가 $5$인 샘플 $1000$개를 생성하고, 이 중 $500$개를 훈련 데이터셋, 나머지를 테스트 데이터셋으로 사용 "],"metadata":{"id":"nsRtZ3KgP5El"}},{"cell_type":"code","source":["def makedata():\n","    n_samples = 1000\n","    X, y = make_friedman1(n_samples = n_samples, n_features = 5, noise=1.0, random_state=100)\n","    return train_test_split(X, y, test_size=0.5, random_state=3)"],"metadata":{"id":"J86hpiLVP8kR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1-6. `sklearn.tree` 모듈의 `DecisionTreeRegressor`와 비교 "],"metadata":{"id":"Q0x5Z48BP--y"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = makedata()\n","\n","# 결정트리 깊이 설정\n","max_depth = 10\n","\n","# depth 0에서 루트노드 생성 \n","treeRoot = TNode(0, X_train, y_train)\n","\n","# 결정트리 학습 \n","Construct_Subtree(treeRoot, max_depth)\n","\n","# 예측 \n","y_hat = np.zeros(len(X_test))\n","\n","for i in range(len(X_test)):\n","    y_hat[i] = Predict(X_test[i], treeRoot)\n","\n","regTree = DecisionTreeRegressor(max_depth = 10, random_state=0)\n","\n","regTree.fit(X_train, y_train)\n","y_hat2 = regTree.predict(X_test)\n","\n","MSE_scratch = np.mean(np.power(y_hat-y_test,2))\n","MSE_scikit = np.mean(np.power(y_hat2-y_test, 2))\n","\n","print(\"사이킷런 결정트리:loss= {:.3f}\".format(MSE_scikit))\n","print(\"직접구현 결정트리:loss= {:.3f}\".format(MSE_scratch))"],"metadata":{"id":"hmYyT1QNQBJn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. 앙상블-배깅(Bagging)\n","\n","* 예측기의 성능을 향상시키기 위해 여러개의 훈련데이터셋 각각을 이용하여 학습시킨 예측기를 결합하여 예측하는 방법을 배깅(Bagging)방식\n","\n","* 배깅방식은 특히 가지치기가 이루어지지 않은 결정트리와 같이 훈련데이터셋의 훈련샘플의 작은 변화에 민감하게 영향을 받는 예측기들을 개선할 때 유용한 방식 \n","\n","> * 회귀문제에 대한 결정트리 예측기의 경우를 예로 들어 배깅방식을 설명하면 다음과 같다. \n",">> * 훈련데이터셋 $\\mathcal D$가 어떤 분포를 따르는 랜덤집합이라하고, $\\mathcal D_1,\\cdots,\\mathcal D_n$가 동일한 분포에 iid라 하자. 각 $\\mathcal D_i\\, (1\\le i \\le n)$를 훈련데이터셋으로 학습시킨 예측기를 $g_{\\mathcal D_i}\\, (1\\le i \\le n)$이라 할 때, \n","$$g_{\\rm{avg}}(\\mathbf x) = \\dfrac 1 n \\sum_{i=1}^n g_{\\mathcal D_i}(\\mathbf x)$$\n","와 같이 예측기 $g_{\\rm{avg}}$를 정의하면 큰 수의 법칙에 따라 예측기 $g_{\\rm avg}$는 $n$이 커질 때 $g^*:=\\rm{E}(g_{\\mathcal D})$로 수렴하게 된다. 이때, 아래 정리에서 알 수 있듯이 서로 다른 훈련데이터셋으로 학습시킨 예측기의 평균값으로 예측을 하면 보다 나은 예측기를 구성할 수 있다.    \n","$$ $$\n","\n","> * 이 때 현실적으로 동일한 분포 iid를 따르는 여러 개의 훈련데이터셋 $\\mathcal D_1,\\cdots,\\mathcal D_n$ 을 얻는 것이 어려움.   \n","이를 극복하기 위해 $m$개의 훈련샘플로 이루어진 하나의 훈련데이터셋이 주어질 때, 이 훈련데이터셋에서 $m$개의 훈련샘플을 복원추출로 뽑는 과정을 $n$번 반복하여 랜덤 훈련데이터셋 $\\mathcal D_1^*,\\cdots, \\mathcal D_n^*$을 구성하고    \n","$$ g_{bag}(\\mathbf x)= \\dfrac 1 n \\sum_{i=1}^n g_{\\mathcal D_i^*}(\\mathbf x)$$\n","와 같이 예측기를 구성하는 것을 부트스트랩을 통해 종합한(bootstrapped aggregated) 예측기 또는 bagged 예측기라고 부른다.   \n","\n"],"metadata":{"id":"nDbv5h0KQIDM"}},{"cell_type":"code","source":["# bag 예측기 구성 (500개의 예측기 구성)\n","\n","n_estimators = 500 \n","bag = np.empty((n_estimators), dtype=object)\n","\n","for i in range(n_estimators):\n","\n","    ####### Empty Module.8 #######\n","    # 복원 추출을 이용하여 랜덤한 훈련 데이터셋 정의 (bootstrap)\n","    ids =                                  # np.random.choice 이용하여 인덱스 먼저 정의, 복원 추출로 진행, size는 len(X_train)만큼 샘플링\n","    X_boot =                               # 인덱스 이용하여 데이터 샘플링\n","    y_boot =                               # 인덱스 이용하여 데이터 샘플링\n","    ##############################\n","\n","    bag[i] = DecisionTreeRegressor()\n","    bag[i].fit(X_boot, y_boot)\n","    \n","####### Empty Module.9 #######\n","\n","yhatbag =                                  # 500개의 예측기를 이용하여 yhatbag 생성 반복문 이용하거나 np의 mean 함수 사용할 것    \n","\n","##############################\n","\n","MSE_bagging = np.mean(np.power(yhatbag-y_test, 2))"],"metadata":{"id":"SnsaR0KTQKfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"사이킷런 결정트리:loss= {:.3f}\".format(MSE_scikit))\n","print(\"직접구현 결정트리:loss= {:.3f}\".format(MSE_scratch))\n","print(\"직접구현-배깅방식:loss= {:.3f}\".format(MSE_bagging))"],"metadata":{"id":"6-aAFox7QLwP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 보고서\n","\n","1. 사이킷런에 구현되어 있는 [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)은 이번 텀프로젝트에서 직접 구현한 `1. 결정 트리(Decision Tree)` 방식과 어떤 차이가 있는지 자유롭게 서술하세요. [1점]\n","\n","    * 파라미터 관점에서 기능이 다른 부분을 위주로 서술하시면 됩니다.\n","\n","2. 사이킷런에 구현되어 있는 [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)은 이번 텀프로젝트에서 구현한 `2. 배깅(Bagging)` 방식과 어떤 차이가 있는지 자유롭게 서술하세요. [1점]\n","\n","    * 의사 결정 나무를 구성하는 과정에서 다른 부분을 위주로 서술하시면 됩니다.\n","\n"],"metadata":{"id":"4X9J-VjfQNTt"}}]}